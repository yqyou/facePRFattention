{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "import scikits.bootstrap as boot\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ttest_rel,wilcoxon\n",
    "import os,math,mat73,sys\n",
    "sys.path.append('utility/')\n",
    "from functions import mybarplot,fisherztrans,myheatmap\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "\n",
    "### set parameters\n",
    "\n",
    "\n",
    "subjs = ['CN040','CN041']\n",
    "hemi = ['lh','rh']\n",
    "tasks = ['face','fixation']\n",
    "roi_labels = ['V1','V2','V3']\n",
    "\n",
    "nsubj = len(subjs)\n",
    "nvoxel = 100\n",
    "ntrial = 1280\n",
    "ntrialperpos = 80\n",
    "npos=16\n",
    "ds0 = 2 # degree\n",
    "\n",
    "cm = np.array([[255, 107, 129],[112, 161, 255]])/255\n",
    "\n",
    "ds_list = np.zeros((npos,npos))\n",
    "for s1 in range(npos):\n",
    "    r1 = s1//4\n",
    "    c1 = s1%4\n",
    "    for s2 in range(s1):\n",
    "        r2 = s2//4\n",
    "        c2 = s2%4\n",
    "        ds_list[s1,s2] = ((r1-r2)**2+(c1-c2)**2)**0.5*ds0\n",
    "d = np.unique(ds_list)[1:]\n",
    "\n",
    "nsample = nsubj * np.max([ds_list[ds_list==dd].shape for dd in d[1:]])\n",
    "\n",
    "ps_list = np.zeros(npos)\n",
    "for s in range(npos):\n",
    "    sr = s//4*2\n",
    "    sc = s%4*2\n",
    "    ps_list[s] = ((5-sr)**2+(5-sc)**2)**0.5\n",
    "posd = np.unique(ps_list)\n",
    "nsample0 = nsubj * np.max([ps_list[ps_list==pp].shape for pp in posd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "data = np.load('data/selectdata.npz',allow_pickle=True)\n",
    "stimposicond_all = data['stimposicond_all']\n",
    "v_sel_all = data['v_sel_all']\n",
    "r2_sel_all = data['r2_sel_all']\n",
    "beta_all = data['beta_sel_all']\n",
    "prf_sel_all = data['prf_sel_all']\n",
    "\n",
    "# stimposicond_all: [tasks * trial x sample]\n",
    "# v_sel_all : roi x sample x voxel, from 0\n",
    "# r2_sel_all : roi x sample x voxel x 2[glmr2,prfr2]\n",
    "# beta_sel_all: task x roi x sample x voxel x trial\n",
    "# prf_sel_all: task x roi x sample x voxel x 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 SVM classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=1,kernel='linear',gamma='scale',degree=3)\n",
    "y = np.hstack([np.zeros(ntrialperpos),np.zeros(ntrialperpos)+1])\n",
    "acc_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "            for s1 in range(npos):\n",
    "                for s2 in range(s1,npos):  \n",
    "                    X = np.hstack([beta[:,:,s1],beta[:,:,s2]]).T\n",
    "                    X = X[:,~np.isnan(X).any(axis=0)]\n",
    "                    scores = cross_val_score(clf,X,y,cv=10,n_jobs=35)\n",
    "                    acc_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = np.array([scores.mean(),scores.mean()])\n",
    "\n",
    "# concat reuslts by distance\n",
    "acc_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            acc_dd = np.array([])\n",
    "            for subj_i in range(len(subjs)):\n",
    "                acc_dd = np.append(acc_dd,acc_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "            acc_d[dd,0:len(acc_dd),roi_i,task_i] = acc_dd\n",
    "\n",
    "\n",
    "# average accuracy across position pairs for each distance for each subject\n",
    "acc_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    acc_d_subj[dd,:,:,:] = np.nanmean(acc_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "\n",
    "np.savez('data/svmacc',acc_ij_all=acc_ij_all,acc_d=acc_d,acc_d_subj=acc_d_subj)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 individual level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 vRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vRF_allv = prf_sel_all # ntask x nroi x nsubj x nvoxel x 8\n",
    "vRF_all = np.nanmedian(vRF_allv,axis=3) # ntask x nroi x nsubj x 8\n",
    "# 0-R2, 1-x, 2-y, 3-ecc, 4-ang, 5-size, 6-expt, 7-gain\n",
    "np.savez('data/vRF',vRF_allv=vRF_allv,vRF_all=vRF_all)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Fano Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the variance to the mean is often called the **Fano factor** and is a measure of the variability in the number of spikes in relation to the mean number of spikes:\n",
    "\n",
    "$$\n",
    "FF(T)≡\\frac{σ_{N(0,T)}^2}{m_{N(0,T)}}\n",
    "$$\n",
    "$$\n",
    "FF_i = \\frac{v_i}{b_i}\n",
    "$$\n",
    "$$\n",
    "FF^n = median(FF_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "FF_p_all = np.zeros((npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npos(s1),nsubj,nroi,ntask]\n",
    "FFv_all = np.zeros((nvoxel,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [nvoxel,npos(s1),nsubj,nroi,ntask]\n",
    "FF2_p_all = np.zeros((npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npos(s1),nsubj,nroi,ntask]\n",
    "FF2v_all = np.zeros((nvoxel,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [nvoxel,npos(s1),nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "                v = np.var(beta[:,:,pos],axis=1) # 100\n",
    "                f = np.mat(beta_mean[:,pos]) # 1 x 100\n",
    "                # 1 x 100\n",
    "                # abs\n",
    "                FF_n = np.multiply(v,abs(1/f))\n",
    "                FFv_all[:,pos,subj_i,roi_i,task_i] = np.array(FF_n)\n",
    "                FF_p_all[pos,subj_i,roi_i,task_i] = np.nanmedian(np.array(FF_n))\n",
    "                # square\n",
    "                FF2_n = np.square(FF_n)\n",
    "                FFv_all[:,pos,subj_i,roi_i,task_i] = np.array(FF2_n)\n",
    "                FF_p_all[pos,subj_i,roi_i,task_i] = np.nanmedian(np.array(FF2_n))\n",
    "\n",
    "# concat data across position\n",
    "FF_all = np.zeros((npos*nsubj,len(roi_labels),len(tasks)))*np.nan\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        c = np.array([])\n",
    "        for subj_i in range(len(subjs)):\n",
    "            c = np.append(c,FF_p_all[:,subj_i,roi_i,task_i])\n",
    "        FF_all[:,roi_i,task_i] = c\n",
    "\n",
    "FF2_all = np.zeros((npos*nsubj,len(roi_labels),len(tasks)))*np.nan\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        c = np.array([])\n",
    "        for subj_i in range(len(subjs)):\n",
    "            c = np.append(c,FF2_p_all[:,subj_i,roi_i,task_i])\n",
    "        FF2_all[:,roi_i,task_i] = c\n",
    "\n",
    "\n",
    "np.savez('data/FF',FF_p_all=FF_p_all,FF_all=FF_all,FFv_all=FFv_all,\n",
    "         FF2_p_all=FF2_p_all,FF2_all=FF2_all,FF2v_all=FF2v_all)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "rv_p_all = np.zeros((npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npos(s1),nsubj,nroi,ntask]\n",
    "rvv_all = np.zeros((nvoxel,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [nvoxel,npos(s1),nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "                rvv_all[:,pos,subj_i,roi_i,task_i] = np.var(beta[:,:,pos],axis=1)\n",
    "                rv_p_all[pos,subj_i,roi_i,task_i] = np.nanmedian(np.var(beta[:,:,pos],axis=1)) # 100\n",
    "                \n",
    "# concat data across position\n",
    "rv_all = np.zeros((npos*nsubj,len(roi_labels),len(tasks)))*np.nan\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        c1 = np.array([])\n",
    "        for subj_i in range(len(subjs)):\n",
    "            c1 = np.append(c1,rv_p_all[:,subj_i,roi_i,task_i])\n",
    "        rv_all[:,roi_i,task_i] = c1\n",
    "\n",
    "np.savez('data/responsevariance',rv_p_all=rv_p_all,rv_all=rv_all,rvv_all=rvv_all)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 noise correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "corr_p_all = np.zeros((npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1), nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "            for s1 in range(npos):\n",
    "                c = np.triu(np.corrcoef(beta[:,:,s1]),1) # 100 x 100 # 取上三角(不包括对角)\n",
    "                corr_p_all[s1,subj_i,roi_i,task_i] = np.nanmedian(c[c!=0])\n",
    "                #corr_p_all[s1,subj_i,roi_i,task_i] = fisherztrans(np.nanmedian(c[c!=0]))  # Fisher Z transformation\n",
    "\n",
    "# concat data across position\n",
    "corr_all = np.zeros((npos*nsubj,len(roi_labels),len(tasks)))*np.nan\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        c = np.array([])\n",
    "        for subj_i in range(len(subjs)):\n",
    "            c = np.append(c,corr_p_all[:,subj_i,roi_i,task_i])\n",
    "        corr_all[:,roi_i,task_i] = c\n",
    "\n",
    "np.savez('data/corr',corr_p_all=corr_p_all,corr_all=corr_all)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 LFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 I_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_lfi(beta_mean_cond,beta_cond,s1,s2,log=False):\n",
    "    # ds\n",
    "    dr = s1//4-s2//4\n",
    "    dc = s1%4-s2%4\n",
    "    ds = ds0 *  (dr**2+dc**2)**0.5\n",
    "    # df & |dfnorm|\n",
    "    df = np.mat(beta_mean_cond[:,s1] - beta_mean_cond[:,s2])\n",
    "    dfnorm = np.linalg.norm(df,axis=1)\n",
    "    # eigenvector & eigenvalue\n",
    "    q1 = np.cov(beta_cond[:,:,s1])\n",
    "    q2 = np.cov(beta_cond[:,:,s2])\n",
    "    q = (q1 + q2)/2 # 100 x 100\n",
    "    w,v = np.linalg.eigh(q)\n",
    "    ullambda = np.nanmean(w)\n",
    "    htlambda = w/ullambda\n",
    "    # lfi\n",
    "    lfi = dfnorm**2/ullambda*np.nansum([(df/dfnorm@v[:,i])**2/htlambda[i] for i in range(len(htlambda))])/(ds)**2\n",
    "    #lfi = df@np.linalg.inv(q)@df.T/(ds)**2\n",
    "    if log:\n",
    "        lfi = np.log(lfi)\n",
    "    return lfi,ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$I=\\frac{df^T*S^{-1}*df}{ds^2}$$\n",
    "$$\\hat I_{bc}=I*\\frac{2T-N-3}{2T-2}-\\frac{2N}{Tds^2}$$\n",
    "\n",
    "$S$ is the empirical covariance for each presented stimulus:\n",
    "$$\n",
    "S_1 = \\frac{1}{T-1} \\displaystyle\\sum^T(\\bold r - \\bold f)^T(\\bold r - \\bold f)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "LFI_bc_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "LFI_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "logLFI_bc_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "logLFI_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "            for s1 in range(npos-1):\n",
    "                for s2 in range(s1+1,npos):\n",
    "                    [LFI_ij,ds] = cal_lfi(beta_mean,beta,s1,s2)\n",
    "                    LFI_bc_ij = LFI_ij*(2*ntrialperpos-nvoxel-3)/(2*ntrialperpos-2)-(2*nvoxel)/(ntrialperpos*(ds)**2)\n",
    "                    LFI_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = LFI_ij\n",
    "                    LFI_bc_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = LFI_bc_ij\n",
    "                    logLFI_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = np.log(LFI_ij)\n",
    "                    logLFI_bc_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = np.log(LFI_bc_ij)\n",
    "\n",
    "                    \n",
    "# concat reuslts by distance\n",
    "LFI_bc_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "LFI_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "logLFI_bc_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "logLFI_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            LFI_bc_dd = np.array([])\n",
    "            LFI_dd = np.array([])\n",
    "            logLFI_bc_dd = np.array([])\n",
    "            logLFI_dd = np.array([])\n",
    "            for subj_i in range(len(subjs)):\n",
    "                LFI_bc_dd = np.append(LFI_bc_dd,LFI_bc_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "                LFI_dd = np.append(LFI_dd,LFI_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "                logLFI_bc_dd = np.append(logLFI_bc_dd,logLFI_bc_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "                logLFI_dd = np.append(logLFI_dd,logLFI_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "            LFI_bc_d[dd,0:len(LFI_bc_dd),roi_i,task_i] = LFI_bc_dd\n",
    "            LFI_d[dd,0:len(LFI_bc_dd),roi_i,task_i] = LFI_dd\n",
    "            logLFI_bc_d[dd,0:len(LFI_bc_dd),roi_i,task_i] = logLFI_bc_dd\n",
    "            logLFI_d[dd,0:len(LFI_bc_dd),roi_i,task_i] = logLFI_dd\n",
    "\n",
    "# average LFI across position pairs for each distance for each subject\n",
    "LFI_bc_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    LFI_bc_d_subj[dd,:,:,:] = np.nanmean(LFI_bc_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "LFI_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    LFI_d_subj[dd,:,:,:] = np.nanmean(LFI_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "logLFI_bc_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    logLFI_bc_d_subj[dd,:,:,:] = np.nanmean(logLFI_bc_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "logLFI_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    logLFI_d_subj[dd,:,:,:] = np.nanmean(logLFI_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "\n",
    "\n",
    "np.savez('data/LFI_bc',LFI_bc_ij_all=LFI_bc_ij_all,LFI_bc_d=LFI_bc_d,LFI_bc_d_subj=LFI_bc_d_subj,\n",
    "         LFI_ij_all=LFI_ij_all,LFI_d=LFI_d,LFI_d_subj=LFI_d_subj,\n",
    "         logLFI_bc_ij_all=logLFI_bc_ij_all,logLFI_bc_d=logLFI_bc_d,logLFI_bc_d_subj=logLFI_bc_d_subj,\n",
    "         logLFI_ij_all=logLFI_ij_all,logLFI_d=logLFI_d,logLFI_d_subj=logLFI_d_subj)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 |df|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "dfnorm_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks))) # [npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta_sel_glmr2\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta_mean[:,pos] = np.nanmean(beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T,axis=1)\n",
    "            for s1 in range(npos):\n",
    "                dfnorm_ij_all[s1,:,subj_i,roi_i,task_i] = np.linalg.norm(beta_mean[:,s1] - beta_mean.T,axis=1)\n",
    "\n",
    "# concat reuslts by distance\n",
    "dfnorm_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            dfnorm_dd = np.array([])\n",
    "            for subj_i in range(len(subjs)):\n",
    "                dfnorm_dd = np.append(dfnorm_dd,dfnorm_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "            dfnorm_d[dd,0:len(dfnorm_dd),roi_i,task_i] = dfnorm_dd\n",
    "\n",
    "# average dfnorm across position pairs for each distance for each subject\n",
    "dfnorm_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    dfnorm_d_subj[dd,:,:,:] = np.nanmean(dfnorm_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "        \n",
    "np.savez('data/dfnorm',dfnorm_ij_all=dfnorm_ij_all,dfnorm_d=dfnorm_d,dfnorm_d_subj=dfnorm_d_subj)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 mean variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "v_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npos,nsubj,nroi,ntask]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)): #[0,1,2,4,5,6,7]:#\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "            for s1 in range(npos):\n",
    "                for s2 in range(s1,npos):\n",
    "                    # eigenvector & eigenvalue\n",
    "                    q1 = np.cov(beta[:,:,s1])\n",
    "                    q2 = np.cov(beta[:,:,s2])\n",
    "                    q = (q1 + q2)/2 # 100 x 100\n",
    "                    '''\n",
    "                    if np.isnan(q).any():\n",
    "                        q=np.array(q)\n",
    "                        q=q[~np.isnan(q).all(axis=1)]\n",
    "                        q=q[:,~np.isnan(q).all(axis=0)]\n",
    "                        q=np.mat(q)\n",
    "                    '''\n",
    "                    w,v = np.linalg.eigh(q)\n",
    "                    ullambda = np.nanmean(w)\n",
    "                    v_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,task_i] = ullambda\n",
    "                \n",
    "# concat results by distance\n",
    "v_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi x task\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            v_dd = np.array([])\n",
    "            for subj_i in range(len(subjs)):\n",
    "                v_dd = np.append(v_dd,v_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "            v_d[dd,0:len(v_dd),roi_i,task_i] = v_dd\n",
    "\n",
    "# average meanvariance across position pairs for each distance for each subject\n",
    "v_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    v_d_subj[dd,:,:,:] = np.mean(v_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "\n",
    "np.savez('data/meanvariance',v_ij_all=v_ij_all,v_d=v_d,v_d_subj=v_d_subj) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.002 sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "sn_ij_all = np.zeros((nvoxel,npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npc,npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "            for s1 in range(npos):\n",
    "                for s2 in range(s1+1,npos):\n",
    "                    # eigenvector & eigenvalue\n",
    "                    q1 = np.cov(beta[:,:,s1])\n",
    "                    q2 = np.cov(beta[:,:,s2])\n",
    "                    q = (q1 + q2)/2 # 100 x 100\n",
    "                    w,v = np.linalg.eigh(q)\n",
    "                    u = np.nanmean(w)\n",
    "                    df = beta_mean[:,s1] - beta_mean[:,s2]\n",
    "                    for i in range(nvoxel):\n",
    "                        dotproduct = np.dot(df,v[:,nvoxel-i-1]) # descend\n",
    "                        df_norm = np.linalg.norm(df)\n",
    "                        sn_ij_all[i,[s1,s2],[s2,s1],subj_i,roi_i,task_i] = (dotproduct/df_norm)**2/(w[nvoxel-i-1]/u)\n",
    "\n",
    "# cumulative\n",
    "sncum_ij_all = np.zeros((nvoxel,npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npc,npos(s1),npos(s2),nsubj,nroi,ntask]\n",
    "for ipc in range(nvoxel):\n",
    "    sncum_ij_all[ipc,:,:,:,:,:] = sn_ij_all[:ipc+1,:,:,:,:,:].sum(axis=0)\n",
    "\n",
    "# sum\n",
    "snsum_ij_all = sn_ij_all.sum(axis=0)\n",
    "\n",
    "# concat results by distance\n",
    "sn_d = np.zeros((nvoxel,len(d),nsample,len(roi_labels),len(tasks)))*np.nan # npc x d x sample x roi\n",
    "sncum_d = np.zeros((nvoxel,len(d),nsample,len(roi_labels),len(tasks)))*np.nan # npc x d x sample x roi\n",
    "snsum_d = np.zeros((len(d),nsample,len(roi_labels),len(tasks)))*np.nan # d x sample x roi\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            sn_dd = np.empty((nvoxel,0))\n",
    "            sncum_dd = np.empty((nvoxel,0))\n",
    "            snsum_dd = np.empty((0))\n",
    "            for subj_i in range(len(subjs)):\n",
    "                sn_dd = np.append(sn_dd,sn_ij_all[:,ds_list==d[dd],subj_i,roi_i,task_i],axis=1)\n",
    "                sncum_dd = np.append(sncum_dd,sncum_ij_all[:,ds_list==d[dd],subj_i,roi_i,task_i],axis=1)\n",
    "                snsum_dd = np.append(snsum_dd,snsum_ij_all[ds_list==d[dd],subj_i,roi_i,task_i])\n",
    "            sn_d[:,dd,0:sn_dd.shape[1],roi_i,task_i] = sn_dd\n",
    "            sncum_d[:,dd,0:sn_dd.shape[1],roi_i,task_i] = sncum_dd\n",
    "            snsum_d[dd,0:sn_dd.shape[1],roi_i,task_i] = snsum_dd\n",
    "\n",
    "# average sn across position pairs for each distance for each subject\n",
    "sn_d_subj = np.zeros((nvoxel,len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # npc x d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    sn_d_subj[:,dd,:,:,:] = np.nanmean(sn_ij_all[:,ds_list==d[dd],:,:,:],axis=1)\n",
    "\n",
    "# average sn across position pairs for each distance for each subject\n",
    "sncum_d_subj = np.zeros((nvoxel,len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # npc x d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    sncum_d_subj[:,dd,:,:,:] = np.nanmean(sncum_ij_all[:,ds_list==d[dd],:,:,:],axis=1)\n",
    "\n",
    "# average sn across position pairs for each distance for each subject\n",
    "snsum_d_subj = np.zeros((len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    snsum_d_subj[dd,:,:,:] = np.nanmean(snsum_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "\n",
    "np.savez('data/sn',sn_ij_all=sn_ij_all,sn_d=sn_d,sn_d_subj=sn_d_subj,\n",
    "         sncum_ij_all=sncum_ij_all,sncum_d=sncum_d,sncum_d_subj=sncum_d_subj,\n",
    "         snsum_ij_all=snsum_ij_all,snsum_d=snsum_d,snsum_d_subj=snsum_d_subj,)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.load('data/sn.npz',allow_pickle=True)\n",
    "sn_ij_all = r['sn_ij_all']\n",
    "sn_d = r['sn_d']\n",
    "sn_d_subj = r['sn_d_subj']\n",
    "sncum_ij_all = r['sncum_ij_all']\n",
    "sncum_d = r['sncum_d']\n",
    "sncum_d_subj = r['sncum_d_subj']\n",
    "snsum_ij_all = r['snsum_ij_all']\n",
    "snsum_d = r['snsum_d']\n",
    "snsum_d_subj = r['snsum_d_subj']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 signal rotation angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta = arccos(\\frac{a b}{|a||b|})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_49412\\2025177016.py:24: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  rot_deg = np.degrees(np.arccos(df_dotproduct/(df_norm_face*df_norm_fix)))\n"
     ]
    }
   ],
   "source": [
    "# calculate\n",
    "srangle_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels)))*np.nan # [npos(s1),npos(s2),nsubj,nroi]\n",
    "for roi_i in range(len(roi_labels)):\n",
    "    for subj_i in range(len(subjs)):\n",
    "        stimposicond_face = stimposicond_all[0,subj_i,:]\n",
    "        stimposicond_fix = stimposicond_all[1,subj_i,:]\n",
    "        # beta for each position (trial-    by-trial)\n",
    "        beta_face = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "        beta_fix = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "        beta_mean_face = np.zeros((nvoxel,npos))*np.nan\n",
    "        beta_mean_fix = np.zeros((nvoxel,npos))*np.nan\n",
    "        for pos in range(npos):\n",
    "            beta_face[:,:,pos] = beta_all[0,roi_i,subj_i,:,stimposicond_face==pos+1].T\n",
    "            beta_fix[:,:,pos] = beta_all[1,roi_i,subj_i,:,stimposicond_face==pos+1].T\n",
    "            beta_mean_face[:,pos] = np.nanmean(beta_face[:,:,pos],axis=1)\n",
    "            beta_mean_fix[:,pos] = np.nanmean(beta_fix[:,:,pos],axis=1)\n",
    "        for s1 in range(npos-1):\n",
    "            for s2 in range(s1,npos):\n",
    "                df_face = beta_mean_face[:,s1]-beta_mean_face[:,s2]\n",
    "                df_fix = beta_mean_fix[:,s1]-beta_mean_fix[:,s2]\n",
    "                df_dotproduct = np.dot(df_face,df_fix)\n",
    "                df_norm_face = np.linalg.norm(df_face)\n",
    "                df_norm_fix = np.linalg.norm(df_fix)\n",
    "                rot_deg = np.degrees(np.arccos(df_dotproduct/(df_norm_face*df_norm_fix)))\n",
    "                srangle_ij_all[[s1,s2],[s2,s1],subj_i,roi_i] = rot_deg\n",
    "\n",
    "# concat results by distance\n",
    "srangle_d = np.zeros((len(d),nsample,len(roi_labels)))*np.nan # d x sample x roi\n",
    "for roi_i in range(len(roi_labels)):\n",
    "    for dd in range(len(d)):\n",
    "        srangle_dd = np.array([])\n",
    "        for subj_i in range(len(subjs)):\n",
    "            srangle_dd = np.append(srangle_dd,srangle_ij_all[ds_list==d[dd],subj_i,roi_i])\n",
    "        srangle_d[dd,0:len(srangle_dd),roi_i] = srangle_dd\n",
    "\n",
    "# average srangle across position pairs for each distance for each subject\n",
    "srangle_d_subj = np.zeros((len(d),nsubj,len(roi_labels)))*np.nan # d x subj x roi\n",
    "for dd in range(len(d)):\n",
    "    srangle_d_subj[dd,:,:] = np.nanmean(srangle_ij_all[ds_list==d[dd],:,:],axis=0)\n",
    "    \n",
    "\n",
    "np.savez('data/srangle',srangle_ij_all=srangle_ij_all,srangle_d=srangle_d,srangle_d_subj=srangle_d_subj)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 PC rotation angle & variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "pcv_ij_all = np.zeros((nvoxel,npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npc,npos,npos,nsubj,nroi,ntask]\n",
    "pcvi_ij_all = np.zeros((nvoxel,npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [npc,npos,npos,nsubj,nroi,ntask]\n",
    "vec_ij_all = np.zeros((nvoxel,nvoxel,npos,npos,nsubj,len(roi_labels),len(tasks)))*np.nan # [eigenvector,npc,npos,npos,nsubj,nroi,ntask]\n",
    "cwangle_ij_all = np.zeros((nvoxel,npos,npos,nsubj,len(roi_labels)))*np.nan # [npc,npos,npos,nsubj,nroi]\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            # beta for each position (trial-by-trial)\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                beta_mean[:,pos] = np.nanmean(beta[:,:,pos],axis=1)\n",
    "            for s1 in range(npos):\n",
    "                for s2 in range(s1+1,npos):\n",
    "                    # eigenvector & eigenvalue\n",
    "                    q1 = np.cov(beta[:,:,s1])\n",
    "                    q2 = np.cov(beta[:,:,s2])\n",
    "                    q = (q1 + q2)/2 # 100 x 100\n",
    "\n",
    "                    w,v = np.linalg.eigh(q)\n",
    "                    sorted_res = sorted(enumerate(w), key=lambda x: x[1],reverse=True)\n",
    "                    sorted_w = [x[1] for x in sorted_res]\n",
    "                    sorted_n = [x[0] for x in sorted_res]\n",
    "                    pcv_ij_all[:len(sorted_w),s1,s2,subj_i,roi_i,task_i] = sorted_w\n",
    "                    pcv_ij_all[:len(sorted_w),s2,s1,subj_i,roi_i,task_i] = sorted_w\n",
    "                    pcvi_ij_all[:len(sorted_w),s1,s2,subj_i,roi_i,task_i] = sorted_w/np.nanmean(w)\n",
    "                    pcvi_ij_all[:len(sorted_w),s2,s1,subj_i,roi_i,task_i] = sorted_w/np.nanmean(w)\n",
    "                    vec_ij_all[:len(sorted_w),:len(sorted_w),s1,s2,subj_i,roi_i,task_i] = v[:,sorted_n]\n",
    "                    vec_ij_all[:len(sorted_w),:len(sorted_w),s2,s1,subj_i,roi_i,task_i] = v[:,sorted_n]\n",
    "\n",
    "for roi_i in range(len(roi_labels)):\n",
    "    for subj_i in range(len(subjs)):\n",
    "        for s1 in range(npos-1):\n",
    "            for s2 in range(s1+1,npos):\n",
    "                for npc in range(nvoxel):\n",
    "                    ev_face = vec_ij_all[:,npc,s1,s2,subj_i,roi_i,0]\n",
    "                    ev_fix = vec_ij_all[:,npc,s1,s2,subj_i,roi_i,1]\n",
    "                    ev_dotproduct = np.dot(ev_face,ev_fix)\n",
    "                    ev_norm_face = np.linalg.norm(ev_face)\n",
    "                    ev_norm_fix = np.linalg.norm(ev_fix)\n",
    "                    crot_deg = np.degrees(np.arccos(ev_dotproduct/(ev_norm_face*ev_norm_fix)))\n",
    "                    cwangle_ij_all[npc,[s1,s2],[s2,s1],subj_i,roi_i] = crot_deg if crot_deg<90 else 180-crot_deg\n",
    "                    #cwangle_ij_all[npc,[s1,s2],[s2,s1],subj_i,roi_i] = crot_deg\n",
    "\n",
    "# concat results by distance\n",
    "cwangle_d = np.zeros((nvoxel,len(d),nsample,len(roi_labels)))*np.nan # pc x d x sample x roi\n",
    "for roi_i in range(len(roi_labels)):\n",
    "    for dd in range(len(d)):\n",
    "        cwangle_dd = np.empty((nvoxel,0))\n",
    "        for subj_i in range(len(subjs)):\n",
    "            cwangle_dd = np.append(cwangle_dd,cwangle_ij_all[:,ds_list==d[dd],subj_i,roi_i],axis=1)\n",
    "        cwangle_d[:,dd,0:cwangle_dd.shape[1],roi_i] = cwangle_dd\n",
    "\n",
    "pcv_d = np.zeros((nvoxel,len(d),nsample,len(roi_labels),len(tasks)))*np.nan # pc x d x sample x roi\n",
    "pcvi_d = np.zeros((nvoxel,len(d),nsample,len(roi_labels),len(tasks)))*np.nan # pc x d x sample x roi\n",
    "for task_i in range(2):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            pcv_dd = np.empty((nvoxel,0))\n",
    "            pcvi_dd = np.empty((nvoxel,0))\n",
    "            for subj_i in range(len(subjs)):\n",
    "                pcv_dd = np.append(pcv_dd,pcv_ij_all[:,ds_list==d[dd],subj_i,roi_i,task_i],axis=1)\n",
    "                pcvi_dd = np.append(pcvi_dd,pcvi_ij_all[:,ds_list==d[dd],subj_i,roi_i,task_i],axis=1)\n",
    "            pcv_d[:,dd,0:pcv_dd.shape[1],roi_i,task_i] = pcv_dd\n",
    "            pcvi_d[:,dd,0:pcvi_dd.shape[1],roi_i,task_i] = pcvi_dd\n",
    "\n",
    "\n",
    "# average data across position pairs for each distance for each subject\n",
    "pcv_d_subj = np.zeros((nvoxel,len(d),nsubj,len(roi_labels),len(tasks)))*np.nan # npc x d x subj x roi x task\n",
    "pcvi_d_subj = np.zeros((nvoxel,len(d),nsubj,len(roi_labels),len(tasks)))*np.nan\n",
    "cwangle_d_subj = np.zeros((nvoxel,len(d),nsubj,len(roi_labels)))*np.nan # npc x d x subj x roi\n",
    "for dd in range(len(d)):\n",
    "    pcv_d_subj[:,dd,:,:,:] = np.nanmean(pcv_ij_all[:,ds_list==d[dd],:,:,:],axis=1)\n",
    "    pcvi_d_subj[:,dd,:,:,:] = np.nanmean(pcvi_ij_all[:,ds_list==d[dd],:,:,:],axis=1)\n",
    "    cwangle_d_subj[:,dd,:,:] = np.nanmean(cwangle_ij_all[:,ds_list==d[dd],:,:],axis=1)\n",
    "\n",
    "np.savez('data/cwangle',pcv_ij_all=pcv_ij_all,cwangle_ij_all=cwangle_ij_all,cwangle_d=cwangle_d,pcv_d=pcv_d,\n",
    "         pcvi_d=pcvi_d,pcvi_ij_all=pcvi_ij_all,\n",
    "         pcv_d_subj=pcv_d_subj,pcvi_d_subj=pcvi_d_subj,cwangle_d_subj=cwangle_d_subj)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 LFI: four mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_lfi_param(beta_mean_cond,beta_cond,s1,s2):\n",
    "    # ds\n",
    "    dr = s1//4-s2//4\n",
    "    dc = s1%4-s2%4\n",
    "    ds = ds0 *  (dr**2+dc**2)**0.5\n",
    "    # df & |dfnorm|\n",
    "    df = np.mat(beta_mean_cond[:,s1] - beta_mean_cond[:,s2])\n",
    "\n",
    "    dfnorm = np.linalg.norm(df,axis=1)\n",
    "    # eigenvector & eigenvalue\n",
    "    q1 = np.cov(beta_cond[:,:,s1])\n",
    "    q2 = np.cov(beta_cond[:,:,s2])\n",
    "    q = (q1 + q2)/2 # 100 x 100\n",
    "\n",
    "    w,v = np.linalg.eigh(q)\n",
    "    ullambda = np.nanmean(w)\n",
    "    htlambda = w/ullambda\n",
    "    return ds,df,dfnorm,v,ullambda,htlambda\n",
    "\n",
    "def cal_lfi(ds,dfnorm,ullambda,df_dfnorm,v,htlambda,bc=True,ntrialperpos=ntrialperpos, nvoxel=nvoxel):\n",
    "    I = 1/(ds)**2*dfnorm**2/ullambda*np.nansum([(df_dfnorm@v[:,i])**2/htlambda[i] for i in range(len(htlambda))])\n",
    "    if bc:\n",
    "        I_bc = I*(2*ntrialperpos-nvoxel-3)/(2*ntrialperpos-2)-(2*nvoxel)/(ntrialperpos*(ds)**2)\n",
    "        return I_bc\n",
    "    else:\n",
    "        return I\n",
    "\n",
    "def cal_lfi_log(ds,dfnorm,ullambda,df_dfnorm,v,htlambda,bc=True,ntrialperpos=ntrialperpos, nvoxel=nvoxel):\n",
    "    I = cal_lfi(ds,dfnorm,ullambda,df_dfnorm,v,htlambda,bc,ntrialperpos, nvoxel)\n",
    "    return np.log(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "Ilog_stw_ij_all = np.zeros((npos,npos,nsubj,len(roi_labels),5))*np.nan # [npos(s1),npos(s2),nsubj,nroi,ntype]\n",
    "for roi_i in range(len(roi_labels)):\n",
    "    for subj_i in range(len(subjs)):\n",
    "        stimposicond_unatt = stimposicond_all[1,subj_i,:]\n",
    "        stimposicond_att = stimposicond_all[0,subj_i,:]\n",
    "        # beta for each position (trial-by-trial)\n",
    "        beta_digit_all = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "        beta_digit_mean_all = np.zeros((nvoxel,npos))*np.nan\n",
    "        beta_face_all = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "        beta_face_mean_all = np.zeros((nvoxel,npos))*np.nan\n",
    "        for pos in range(npos):\n",
    "            beta_digit_all[:,:,pos] = beta_all[1,roi_i,subj_i,:,stimposicond_unatt==pos+1].T\n",
    "            beta_digit_mean_all[:,pos] = np.nanmean(beta_digit_all[:,:,pos],axis=1)\n",
    "            beta_face_all[:,:,pos] = beta_all[0,roi_i,subj_i,:,stimposicond_att==pos+1].T\n",
    "            beta_face_mean_all[:,pos] = np.nanmean(beta_face_all[:,:,pos],axis=1)\n",
    "        for s1 in range(npos-1):\n",
    "            for s2 in range(s1+1,npos):\n",
    "                ds,df_digit,dfnorm_digit,v_digit,ullambda_digit,htlambda_digit = cal_lfi_param(beta_digit_mean_all,beta_digit_all,s1,s2)\n",
    "                ds,df_face,dfnorm_face,v_face,ullambda_face,htlambda_face = cal_lfi_param(beta_face_mean_all,beta_face_all,s1,s2)\n",
    "                # I_digit   I_se    I_ms I_sr,mw  I_face\n",
    "                Ilog_stw_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,0] = cal_lfi_log(ds,dfnorm_digit,ullambda_digit,df_digit/dfnorm_digit,v_digit,htlambda_digit,bc=False)\n",
    "                Ilog_stw_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,1] = cal_lfi_log(ds,dfnorm_face,ullambda_digit,df_digit/dfnorm_digit,v_digit,htlambda_digit,bc=False)\n",
    "                Ilog_stw_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,2] = cal_lfi_log(ds,dfnorm_digit,ullambda_face,df_digit/dfnorm_digit,v_digit,htlambda_digit,bc=False)\n",
    "                Ilog_stw_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,3] = cal_lfi_log(ds,dfnorm_digit,ullambda_digit,df_face/dfnorm_face,v_face,htlambda_face,bc=False)\n",
    "                Ilog_stw_ij_all[[s1,s2],[s2,s1],subj_i,roi_i,4] = cal_lfi_log(ds,dfnorm_face,ullambda_face,df_face/dfnorm_face,v_face,htlambda_face,bc=False)\n",
    "\n",
    "i_types = ['digit','se','ms','srmw','face']\n",
    "# concat reuslts by distance\n",
    "Ilog_stw_d = np.zeros((len(d),nsample,len(roi_labels),5))*np.nan # d x sample x roi x type\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for dd in range(len(d)):\n",
    "            Ilog = {'digit':np.array([]),'se':np.array([]),'ms':np.array([]),'srmw':np.array([]),'face':np.array([])}\n",
    "            for subj_i in range(len(subjs)):\n",
    "                for i in range(5):\n",
    "                    Ilog[i_types[i]] = np.append(Ilog[i_types[i]],Ilog_stw_ij_all[ds_list==d[dd],subj_i,roi_i,i])\n",
    "                    Ilog_stw_d[dd,0:len(Ilog[i_types[i]]),roi_i,i] = Ilog[i_types[i]]\n",
    "\n",
    "deltaIlog_stw_d = Ilog_stw_d[:,:,:,1:]-Ilog_stw_d[:,:,:,[0]] # d x sample x roi x type\n",
    "\n",
    "Ilog_stw_d_subj = np.zeros((len(d),nsubj,len(roi_labels),5))*np.nan # d x subj x roi x task\n",
    "for dd in range(len(d)):\n",
    "    Ilog_stw_d_subj[dd,:,:,:] = np.nanmean(Ilog_stw_ij_all[ds_list==d[dd],:,:,:],axis=0)\n",
    "deltaIlog_stw_d_subj = Ilog_stw_d_subj[:,:,:,1:]-Ilog_stw_d_subj[:,:,:,[0]]# d x subj x roi x type\n",
    "\n",
    "np.savez('data/LFI_separate',Ilog_stw_d=Ilog_stw_d,Ilog_stw_ij_all=Ilog_stw_ij_all,deltaIlog_stw_d=deltaIlog_stw_d,\n",
    "         Ilog_stw_d_subj=Ilog_stw_d_subj,deltaIlog_stw_d_subj=deltaIlog_stw_d_subj)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 other analysis (supplementary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 signal & noise correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "signal correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal corr\n",
    "# calculate\n",
    "signal_corr_all = np.zeros([math.comb(nvoxel,2),len(roi_labels),len(tasks),nsubj])*np.nan\n",
    "for task_i in range(len(tasks)):\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(nsubj):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            for pos in range(npos):\n",
    "                beta_mean[:,pos] = np.nanmean(beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T,axis=1)\n",
    "            sc = np.corrcoef(beta_mean[:,:]) # 100 x 100\n",
    "            signal_corr_all[:,roi_i,task_i,subj_i] = sc[np.triu_indices(nvoxel,k=1)]\n",
    "np.savez('data/sig_corr',signal_corr_all=signal_corr_all)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noise correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate\n",
    "noise_p_corr_all = np.zeros([npos,math.comb(nvoxel,2),len(roi_labels),len(tasks),nsubj])*np.nan\n",
    "for subj_i in range(nsubj):\n",
    "    for task_i in range(len(tasks)):\n",
    "        for roi_i in range(len(roi_labels)):\n",
    "            stimposicond = stimposicond_all[task_i,subj_i,:]\n",
    "            beta_mean = np.zeros((nvoxel,npos))*np.nan\n",
    "            beta = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            for pos in range(npos):\n",
    "                beta[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond==pos+1].T\n",
    "                nc = np.corrcoef(beta[:,:,pos]) # 100 x 100\n",
    "                noise_p_corr_all[pos,:,roi_i,task_i,subj_i] = nc[np.triu_indices(nvoxel,k=1)]\n",
    "noise_corr_all = np.nanmedian(noise_p_corr_all,axis=0)\n",
    "np.savez('data/noise_corr',noise_p_corr_all=noise_p_corr_all,noise_corr_all=noise_corr_all)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 shuffle correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runsvm(pi,clf,y,sfl_n,sfl_type='ori',save=True):\n",
    "    # sfl_type: 'ori' or 'sfld'\n",
    "    acc_ij_face = np.zeros((sfl_n,npos,npos,nsubj,len(roi_labels)))*np.nan # [nshuffle,npos(s1),npos(s2),nsubj,nroi]\n",
    "    idx0 = np.arange(0,ntrial)\n",
    "    task_i = 0\n",
    "    for roi_i in range(len(roi_labels)):\n",
    "        for subj_i in range(len(subjs)):\n",
    "            print('pi{},roi{},subj{}'.format(pi,roi_i,subj_i))\n",
    "            stimposicond0 = stimposicond_all[task_i,subj_i,:].copy() # trial\n",
    "            beta0 = np.zeros((nvoxel,ntrialperpos,npos))*np.nan # voxel x trial x position\n",
    "            for pos in range(npos):\n",
    "                beta0[:,:,pos] = beta_all[task_i,roi_i,subj_i,:,stimposicond0==pos+1].T  \n",
    "            # shuffle\n",
    "            beta_sfl = beta0.copy()\n",
    "            for i in tqdm(range(sfl_n)):\n",
    "                if sfl_type == 'ori':\n",
    "                    for pos in range(npos):\n",
    "                        idx = np.arange(0,ntrialperpos)\n",
    "                        np.random.shuffle(idx)\n",
    "                        beta_sfl[:,:,pos] = beta0[:,idx,pos]      \n",
    "                elif sfl_type == 'sfld':\n",
    "                    for pos in range(npos):\n",
    "                        beta_sfl[:,:,pos] = beta0[np.arange(nvoxel)[:, None], np.argsort(np.random.rand(nvoxel, ntrialperpos), axis=1),pos]\n",
    "                # classification\n",
    "                for s1 in range(npos):\n",
    "                    for s2 in range(s1,npos):  \n",
    "                        X = np.hstack([beta_sfl[:,:,s1],beta_sfl[:,:,s2]]).T\n",
    "                        X = X[:,~np.isnan(X).any(axis=0)]\n",
    "                        scores = cross_val_score(clf,X,y,cv=10,n_jobs=35)\n",
    "                        acc_ij_face[i,[s1,s2],[s2,s1],subj_i,roi_i] = np.array([scores.mean(),scores.mean()])\n",
    "    return acc_ij_face\n",
    "\n",
    "def parallel_shuffle(clf,y,sfl_n,sfl_type='ori',save=True):\n",
    "    results = Parallel(n_jobs=-1)(delayed(runsvm)(pi,clf,y,sfl_n,sfl_type,save) for pi in range(npall))\n",
    "    acc_ij_face = np.concatenate(results, axis=0)\n",
    "\n",
    "    # save                        \n",
    "    if save: np.savez('data/svmacc_shuffle_'+sfl_type,acc_ij_face=acc_ij_face)  \n",
    "\n",
    "    return acc_ij_face\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_49412\\3670271744.py\", line 15, in runsvm\nTypeError: 'module' object is not callable\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m sfl_idx = \u001b[32m1\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# run \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m acc_ij_face = \u001b[43mparallel_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43msfl_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43msfl_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43msfl_types\u001b[49m\u001b[43m[\u001b[49m\u001b[43msfl_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mparallel_shuffle\u001b[39m\u001b[34m(clf, y, sfl_n, sfl_type, save)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_shuffle\u001b[39m(clf,y,sfl_n,sfl_type=\u001b[33m'\u001b[39m\u001b[33mori\u001b[39m\u001b[33m'\u001b[39m,save=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     34\u001b[39m     npall=\u001b[32m5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunsvm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43msfl_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43msfl_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpall\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     acc_ij_face = np.concatenate(results, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# save                        \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MySoftwares\\anaconda3\\envs\\py311\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# set\n",
    "import tqdm as tqdm\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "clf = svm.SVC(C=1,kernel='linear',gamma='scale',degree=3)\n",
    "y = np.hstack([np.zeros(ntrialperpos),np.zeros(ntrialperpos)+1])\n",
    "sfl_total = 1000\n",
    "npall = 20\n",
    "sfl_n = int(sfl_total/npall)\n",
    "sfl_types = ['ori','sfld']\n",
    "sfl_idx = 1\n",
    "\n",
    "# run \n",
    "acc_ij_face = parallel_shuffle(clf,y,sfl_n,sfl_type=sfl_types[sfl_idx],save=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
